Exploratory Data Analysis (EDA) 

1) Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


2) Loading Dataset
df = pd.read_excel("filename.xlsx") // getting file into notebook
df = pd.read_csv("filename.csv")

For large files:
df.memory_usage(deep=True)


3) Basic Data Exploration

   i. head of dataset
   ii. shape of dataset
  iii. info of dataset
   iv. summary of dataset

df.head() - first 5 columns of the file
df.shape (attribute) - dimensions ( rows and columns ) 
df.info() - each column data types, missing values,etc
df.describe() - mean, standard deviation, minimum value, maximum value,etc
              - Used for statistical information of numeriacal values
dups= df.duplicated() - gives the duplicated columns
dups.sum() - no. of duplicated values
df[dups]  - prints dupliacted values

df.drop_duplicates(inplace=true) - drops duplicated values not from csv or excel but from the particular notebook

df.shape again to check if values have been deleted

Handling Duplicates:
dups = df.duplicated()
dups.sum()       # number of duplicates
df[dups]         # print duplicate rows
df.drop_duplicates(inplace=True)


4) Outlier Treatment

Boxplot Visualization:

i. To check for outliers, we will plot box plots
df.boxplot(column=['columnname'])
plt.show()
sns.boxplot(x=df['columnname'])

Outlier Detection Function (IQR):

ii. User defined function for finding the lower and upper range variable so that outlier can be treated.

def remove_outlier(col):    // IQR: Interquartile Range
sorted(col)
Q1,Q3=col.quantile([0.25,0.75])
IQR=Q3-Q1
lower_range=Q1-(1.5 * IQR)
upper_range=Q3+(1.5 * IQR)
return lower_range,upper_range

-> after treating outliers check again through boxplot
df.boxplot(column=['columnname'])
plt.show() - shows visualization chart


5) Missing Values
Check Missing:
df.isnull().sum()
df.isnull().mean() * 100   # percentage

Strategies:
- Numerical (with outliers): replace with Median
- Numerical (no outliers): replace with Mean
- Categorical: replace with Mode


Note: For Normal distribution, missing values can be replaced with mean or mode or median because in case of normal distribution all these 3 are same. 

checking datatype to classify into categorical/ numerical

df[df.isnull().sum()[df.isnull().sum()>0].index].dtypes

Replace Example:
median_val = df["num_col"].median()
df["num_col"].replace(np.nan, median_val, inplace=True)

mode_val = df["cat_col"].mode().values[0]
df["cat_col"].replace(np.nan, mode_val, inplace=True)

-> After replacing again check if still there are missing values

check for missing value in a per column wise
   df.isnull().sum()

6) Univariate Analysis
Numerical:
sns.histplot(df['num_col'], bins=20) // Histogram of that column

Categorical:
sns.countplot(df["columnname"],hue=df["columnname"]
 // hue: Apart of an graph

7) Bivariate Analysis
Numerical vs Numerical:
sns.pairplot(df)
plt.show()
df.corr()

Numerical vs Categorical:
sns.boxplot(x='cat_col', y='num_col', data=df)
sns.barplot(x='cat_col', y='num_col', data=df)


8) Correlation Heatmap

plt.figure(figsize=(12,7))
sns.heatmap(df.corr(), annot=True , fmt=.2f , cmap='Blues')
annot=True // Shows the values
fmt=.2f // after point no. of values
if dark color: more correlation
if light color: less correlation
plt.show()

9) Normalizing and Scaling

-> Scaling the data, essentially returns the z-scores of every attribute

- StandardScaler normalizes the data using (x-mean)/SD (only for numerical)

from sklearn.preprocessing import StandardScaler
std_scale = StandardScaler()
std_scale

df['colunname']= std_scale.fit_tranform(df[['columnname']])

df.head() // now the values will be in particular range

10) Encoding
- used to create dummy variables to replace the categories in a categorical variables into features of each category and represting it using 1 or 0 based on the presence or absence of the categorical value in record. 

dummies= get_dummies(df["columnname","columnname"]],columns["columnname",columnname"],drop_first=true).head()

columns= ["columnname","columnname"]
df=pd.concat([df, dummies], axis=1)

dropping original column from df
df.drop(columns,axis=1,inplace=True)

df.head() // Now every thing is in 0 or 1
